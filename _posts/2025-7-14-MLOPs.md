---
layout: post
title: "Machine Learning Operations(MLOps): Deploying a ML Model with FastAPI"
date: "2025-07-11 18:15:00 +0300"
tags: [Machine Learning, FastAPI, Google Colab, API, Deployment, Python, Data Science, MLOps, ngrok]
category: [Machine Learning, MLOps, FastAPI, Python, Google Colab, ngrok, API Deployment,Housing Price Prediction, Data Science, Model Serving, Scikit-learn, Regression]
description: "Learn how to build an end-to-end machine learning pipeline for California housing price prediction, covering data preprocessing, model training, and real-time API deployment using FastAPI and ngrok in Google Colab."
comments: True
slug: "mlops-fastapi-colab-housing-prediction"
---


1. ## **Introduction**

   In this assignment, I built an end-to-end machine learning workflow to predict housing prices using the California Housing dataset. The focus was on applying key concepts covered in class, including preprocessing, cross-validation, hyperparameter tuning, pipeline construction, and model deployment. The California Housing dataset is a well-known regression dataset that contains information collected from the 1990 California census and is available directly through scikit-learn.
   For this dataset, there was the **Target Variable**: Median house value (in $100,000s), and there were also **8 Features**:
	1. MedInc: median income in block group
	2. HouseAge: median house age in block group
	3. AveRooms: average number of rooms per household
	4. AveBedrms: average number of bedrooms per household
	5. Population: block group population
	6. AveOccup: average number of household members
	7. Latitude: block group latitude
	8. Longitude: block group longitude

2. ## **Tasks to be completed**

   1. ### **Loading the dataset**

      I first imported the required libraries, which were essential for this task to be accomplished, then I loaded the California housing dataset using the command fetch\_california\_housing(). This separates features into X and the target(house values) into y. The return\_X\_y=**True** argument ensures this split occurs. as\_frame=**True**, returns X as a pandas Dataframe and y as a pandas series, facilitating data manipulation.

      ![][image1] *Figure 1: Importing the libraries*


      ![][image2] *Figure 2: Importing the libraries continued*


      ![][image3] *Figure 3: Loading the dataset*


   2. ### **Splitting the data**

      The next step was to split the dataset into training and testing sets. X\_train and y\_train are used for model training, while X\_test and y\_test are used for evaluation. test\_size=0.2 allocates 20% of the data for testing. random\_state=42 ensures the split is reproducible, making results consistent across runs.

      ![][image4] *Figure 4: Splitting the dataset*

   3. ### **Preprocessing**

      The next step involved defining a preprocessing pipeline for numerical features. numeric\_features identifies all columns in X for transformation. The Pipeline first uses SimpleImputer to fill any missing values with the mean. Then, StandardScaler scales these features to have a zero mean and unit variance, preparing them for model training.

      ![][image5] *Figure 5: Preprocessing*


   4. ### **Combine Preprocessing using ColumnTransformer**

      This next step creates a ColumnTransformer named preprocessor. It applies specific transformations to designated columns. Here, it applies the numeric\_transformer pipeline (imputation and scaling) to all numeric\_features identified earlier. This setup ensures that different preprocessing steps can be applied to different subsets of features within the overall data pipeline.
      ![][image6] *Figure 6: Combine preprocessing using Column Transformer*


   5. ### **Build Pipeline**

      In this next step, I constructed the main machine learning Pipeline. It sequentially combines the defined preprocessor (for data cleaning and scaling) with the KNeighborsRegressor model. This structure ensures that all the data undergoes the necessary preprocessing steps before being fed into the KNN algorithm for regression, streamlining the entire workflow.

      ![][image7] *Figure 7: Build the pipeline*


   6. ### **Hyperparameter grid definition**

      The param\_grid dictionary defines the hyperparameters to optimize for the KNeighborsRegressor. Hyperparameters are settings that are configured before the learning process begins, controlling how the model learns from the data. It specifies different values for the number of neighbors (knn\_\_n\_neighbors), weighting schemes(knn\_\_weights), and Minkowski distance metric(knn\_\_p). This grid will be explored by GridSearchCV to find the best combination for the model.

      ![][image8] *Figure 8: Hyperparameter grid definition*


   7. ### **GridSearchCV with 5-fold cross-validation**

      In this step, GridSearchCV is initialized to systematically search for optimal hyperparameters. It uses the defined pipeline as the estimator and explores combinations from param\_grid. With cv=5, it performs 5-fold-cross-validation, optimizing for the R^2  score(scoring='r2').  n\_jobs=\-1 enables parallel processing for efficiency.
      ![][image9] *Figure 9: GridSearchCV with 5-fold cross-validation*


   8. ### **Model fitting**

      In this step, the code initiates the training process for the GridSearchCV object. It fits the pipeline to the training data (X\_train, y\_train). During the process, GridSearchCV performs cross-validation across all specified hyperparameter combinations. It then identifies the best-performing model based on the chosen scoring metric.

      ![][image10] *Figure 10: Model fitting*


   9. ### **Model evaluation**

      This step starts by retrieving the best\_model from the GridSearchCV process, representing the optimal configuration found. This best-performing model is then used to generate y\_pred predictions on the entirely unseen X\_test data, ensuring an unbiased performance assessment. In addition to that, standard regression evaluation metrics are computed. The R2 score measures the proportion of variance explained, and Mean Squared Error(MSE) quantifies average squared differences. And Root Mean Squared Error(RMSE), derived from MSE, provides error in the original units. These metrics help to understand the model’s predictive accuracy and generalization capability.

      ![][image11] *Figure 11: Model evaluation*


   10. ### **Printing results**

       This code block prints the key results of the model training and evaluation. It displays the Best Parameters found by GridSearchCV and the corresponding Best CV R^2  Score from cross-validation. It then presents the model’s performance on the unseen test set, showing the Test R^2 Score, Test MSE, and Test RMSE, which were all formatted to 4 decimal places for clarity.

       ![][image12] *Figure 12: Printing results*


   11. ### **Saving the pipeline**

       This part is crucial for model persistence, saving the best\_model to disk. Using Python’s pickle module, the entire trained pipeline, is serialized into a binary file named california\_knn\_pipeline.pkl. This enables the model to be loaded efficiently later for new predictions without the need for time-consuming retraining. A confirmation message is printed to say the final pipeline has been saved.

       ![][image13] *Figure 13: Saving the pipeline*


   12. ### **FastAPI app creation**

       In this section, the code is used to define and run a machine learning prediction API using FastAPI. It begins by initializing the FastAPI application and securely loading the pre-trained california\_knn\_pipeline.pkl model globally for efficient predictions. Input data for the prediction endpoint is rigorously validated using a Pydantic BaseModel, ensuring data integrity. The API exposes a root endpoint (/) for health checks and a POST endpoint (/predict) that handles incoming feature data, performs model inference, and returns the predicted house value. Error handling is implemented using FastAPI’s HTTPException for clear client feedback. Finally, the FastAPI application is launched using Uvicorn in a separate thread, and an ngrok tunnel is established (with a securely prompted authentication token) to provide a public ngrok.io URL, making the API accessible from outside the Google Colab environment for testing and integration. The test was done using Postman to send a POST request, and we got a response 200 OK with a prediction response.

       ![][image14] *Figure 14: FastAPI app creation*

       ![][image15] *Figure 15: FastAPI app creation continued*

       ![][image16] *Figure 16: FastAPI app creation continued*

       ![][image17] *Figure 17: FastAPI app creation continued*

       ![][image18] *Figure 18: FastAPI app creation continued*


       ![][image19] *Figure 19: FastAPI app API homepage*

       ![][image20] *Figure 20: FastAPI app Postman POST Request and Response*


       To interpret the Postman results, we see that based on a block group having a median income of $38750, houses that are 30 years old, 5.5 rooms per household on average, 1.1 bedrooms per household on average, a population of 1500, an average of 5 occupants per household, and its latitude being 34.05 and longitude being \-118.25, the machine learning model predicts that the median house value in that area is approximately $188,389.85.


   13. ### **Sharing the work**

       ### **Link to Google Colab notebook**



       [Uel Kariuki Google Colab Notebook](https://colab.research.google.com/drive/1WRjRlSLSikdmz1c6_XLoMcyGfOkz73Ix?usp=sharing)


3. ## **Conclusion** {#conclusion}

   This project, which focused on building an end-to-end machine learning workflow for California housing price prediction, provided an invaluable hands-on experience. Moving from initial dataset loading and preprocessing to model training, hyperparameter tuning and finally, deploying the model as a real time prediction API using FastAPI, I gained significant insights into practical MLOps principles. This whole process deepened my understanding of how to construct resilient machine learning pipelines, expose model predictions via web services, and ensure seamless integration. The skills acquired throughout this project, particularly in API development and deployment for machine learning models, will enhance my skillset and strengthen my portfolio for future Data and AI opportunities.


[image1]: /assets/images/Projects/mlops_images/1.0_mlops.png
[image2]: /assets/images/Projects/mlops_images/1.1_mlops.png
[image3]: /assets/images/Projects/mlops_images/1.2_mlops.png
[image4]: /assets/images/Projects/mlops_images/1.3_mlops.png
[image5]: /assets/images/Projects/mlops_images/1.4_mlops.png
[image6]: /assets/images/Projects/mlops_images/1.5_mlops.png
[image7]: /assets/images/Projects/mlops_images/1.6_mlops.png
[image8]: /assets/images/Projects/mlops_images/1.7_mlops.png
[image9]: /assets/images/Projects/mlops_images/1.8_mlops.png
[image10]: /assets/images/Projects/mlops_images/1.9_mlops.png
[image11]: /assets/images/Projects/mlops_images/2.0_mlops.png
[image12]: /assets/images/Projects/mlops_images/2.1_mlops.png
[image13]: /assets/images/Projects/mlops_images/2.2_mlops.png
[image14]: /assets/images/Projects/mlops_images/2.3_mlops.png
[image15]: /assets/images/Projects/mlops_images/2.4_mlops.png
[image16]: /assets/images/Projects/mlops_images/2.5_mlops.png
[image17]: /assets/images/Projects/mlops_images/2.6_mlops.png
[image18]: /assets/images/Projects/mlops_images/2.7_mlops.png
[image19]: /assets/images/Projects/mlops_images/2.8_mlops.png
[image20]: /assets/images/Projects/mlops_images/2.9_mlops.png
